{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1: Information retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will apply basic techniques from information retrieval to implement the core of a minimalistic search engine. The data for this lab consists of a collection of app descriptions scraped from the [Google Play Store](https://play.google.com/store/apps?hl=en). From this collection, your search engine should retrieve those apps whose descriptions best match a given query under the vector space model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before starting with this lab, make sure that you have read the [Rules for hand-in assignments](https://www.ida.liu.se/~TDDE16/exam.en.shtml#handins) and the [Policy on cheating and plagiarism](https://www.ida.liu.se/~TDDE16/exam.en.shtml#cheating).** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The app descriptions come in the form of a compressed [JSON](https://en.wikipedia.org/wiki/JSON) file. Start by loading this file into a [Pandas](https://pandas.pydata.org) [DataFrame](https://pandas.pydata.org/pandas-docs/stable/getting_started/dsintro.html#dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import pandas as pd\n",
    "\n",
    "with bz2.open('app-descriptions.json.bz2') as source:\n",
    "    df = pd.read_json(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pandas, a DataFrame is a table with indexed rows and labelled columns of potentially different types. Data in a DataFrame can be accessed in various ways, including by row and by column. To give an example, the code in the next cell shows rows 200‚Äì204:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1614\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Brick Breaker Star: Space King</td>\n",
       "      <td>Introducing the best Brick Breaker game that e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Brick Classic - Brick Game</td>\n",
       "      <td>Classic Brick Game!\\n\\nBrick Classic is a popu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Bricks Breaker - Glow Balls</td>\n",
       "      <td>Bricks Breaker - Glow Balls is a addictive and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Bricks Breaker Quest</td>\n",
       "      <td>How to play\\n- The ball flies to wherever you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Brothers in Arms¬Æ 3</td>\n",
       "      <td>Fight brave soldiers from around the globe on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               name  \\\n",
       "200  Brick Breaker Star: Space King   \n",
       "201      Brick Classic - Brick Game   \n",
       "202     Bricks Breaker - Glow Balls   \n",
       "203            Bricks Breaker Quest   \n",
       "204             Brothers in Arms¬Æ 3   \n",
       "\n",
       "                                           description  \n",
       "200  Introducing the best Brick Breaker game that e...  \n",
       "201  Classic Brick Game!\\n\\nBrick Classic is a popu...  \n",
       "202  Bricks Breaker - Glow Balls is a addictive and...  \n",
       "203  How to play\\n- The ball flies to wherever you ...  \n",
       "204  Fight brave soldiers from around the globe on ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df))\n",
    "df[200:205]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are two labelled columns: `name` (the name of the app) and `description` (a textual description). The code in the next cell shows how to acess fields from the description column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['name', 'description'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Ôªø10000000 is a Dungeon Crawling Puzzle RPG Mat...\n",
       "1    I 1177 V√•rdguidens app f√•r du tillg√•ng till 11...\n",
       "2    Need counting games for kids & drawing for tod...\n",
       "3    Beautiful and simple music application for tod...\n",
       "4    A Fun and intuitive numbers game for your baby...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.keys())\n",
    "df['description'][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to implement a preprocessor for your search engine. In the vector space model, *preprocessing* refers to any kind of transformation that is applied before a text is vectorized. Here you can restrict yourself to a very simple preprocessing: tokenization, stop word removal, and lemmatization.\n",
    "\n",
    "To implement your preprocessor, you can use [spaCy](https://spacy.io). Make sure that you read the [Linguistic annotations](https://spacy.io/usage/spacy-101#annotations) section of the spaCy&nbsp;101; that section contains all the information that you need for this problem (and more).\n",
    "\n",
    "Implement your preprocessor by completing the skeleton code in the next cell, adding additional code as you feel necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def preprocess(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\", \"ner\", \"entity_linker\", \"entity_ruler\"])\n",
    "    \n",
    "    return [token.lemma_.lower() for token in nlp(text) \\\n",
    "            if not(token.is_stop) and token.lemma_.isalpha()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should conform to the following specification:\n",
    "\n",
    "<strong>preprocess</strong> (<em>text</em>)\n",
    "\n",
    "> Preprocesses given text by tokenizing it, removing any stop words, replacing each remaining token with its lemma (base form), and discarding all lemmas that contain non-alphabetical characters. Returns the list of remaining lemmas (represented as strings).\n",
    "\n",
    "**Tip:** To speed up the preprocessing, you can disable loading those spaCy components that you do not need, such as the parser, and named entity recognizer. See [here](https://spacy.io/usage/processing-pipelines#disabling) for more information about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'look', 'buy', 'startup', 'billion']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess('Apple is looking at buying U.K. startup for $1 billion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give the following output:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "['Apple', 'look', 'buy', 'startup', 'billion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to vectorize the data ‚Äì and more specifically, to map each app description to a tf‚Äìidf vector. For this you can use the [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) class from [scikit-learn](https://scikit-learn.org/stable/). Make sure to specify your preprocessor from the previous problem as the `tokenizer` &ndash; not the `preprocessor`! &ndash; for the vectorizer. (In scikit-learn parlance, the `preprocessor` handles string-level preprocessing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=preprocess)\n",
    "X = vectorizer.fit_transform(df['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1614, 21427)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should show the dimensions of the matrix `X` to be 1614 √ó 21427."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Retrieving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the search engine, your last task is to write a function that returns the most relevant app descriptions for a given query. An easy way to do solve this task is to use scikit-learn&rsquo;s [NearestNeighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html) class. That class implements unsupervised nearest neighbours learning, and allows you to easily find a predefined number of app descriptions whose vector representations are closest to the query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def search(query):\n",
    "    neigh = NearestNeighbors(n_neighbors=10, radius=0.4)\n",
    "    neigh.fit(X)\n",
    "    query_idf = vectorizer.transform([query])\n",
    "    neighbours = neigh.kneighbors(query_idf, return_distance=False)\n",
    "    \n",
    "    return df.iloc[[i for i in neighbours[0]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should conform to the following specification:\n",
    "\n",
    "<strong>search</strong> (<em>query</em>)\n",
    "\n",
    "> Returns the 10 app descriptions most similar (in terms of cosine similarity) to the given query as a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>Subway Surfers</td>\n",
       "      <td>DASH as fast as you can! \\nDODGE the oncoming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>Train Conductor World</td>\n",
       "      <td>Master and manage the chaos of international r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>Tiny Rails</td>\n",
       "      <td>All aboard for an adventure around the world!\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>Subway Princess Runner</td>\n",
       "      <td>Subway princess runner, Bus run, forest rush w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>No Humanity - The Hardest Game</td>\n",
       "      <td>2M+ Downloads All Over The World!\\n\\n* IGN Nom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>Train for Animals - BabyMagica free</td>\n",
       "      <td>üöÇ BabyMagica \"Train for Animals\" is a educatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>Rush</td>\n",
       "      <td>Are you ready for a thrilling ride?\\n\\nRush th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>LEGO¬Æ DUPLO¬Æ Train</td>\n",
       "      <td>All aboard! Driving the colorful LEGO¬Æ DUPLO¬Æ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>Virus War - Space Shooting Game</td>\n",
       "      <td>Warning! Virus invasion! Destroy them with you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Boxing Star</td>\n",
       "      <td>Go for the K.O.!\\nMake Your Opponent See Stars...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     name  \\\n",
       "1301                       Subway Surfers   \n",
       "1428                Train Conductor World   \n",
       "1394                           Tiny Rails   \n",
       "1300               Subway Princess Runner   \n",
       "998        No Humanity - The Hardest Game   \n",
       "1429  Train for Animals - BabyMagica free   \n",
       "1168                                 Rush   \n",
       "786                    LEGO¬Æ DUPLO¬Æ Train   \n",
       "1465      Virus War - Space Shooting Game   \n",
       "192                           Boxing Star   \n",
       "\n",
       "                                            description  \n",
       "1301  DASH as fast as you can! \\nDODGE the oncoming ...  \n",
       "1428  Master and manage the chaos of international r...  \n",
       "1394  All aboard for an adventure around the world!\\...  \n",
       "1300  Subway princess runner, Bus run, forest rush w...  \n",
       "998   2M+ Downloads All Over The World!\\n\\n* IGN Nom...  \n",
       "1429  üöÇ BabyMagica \"Train for Animals\" is a educatio...  \n",
       "1168  Are you ready for a thrilling ride?\\n\\nRush th...  \n",
       "786   All aboard! Driving the colorful LEGO¬Æ DUPLO¬Æ ...  \n",
       "1465  Warning! Virus invasion! Destroy them with you...  \n",
       "192   Go for the K.O.!\\nMake Your Opponent See Stars...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('dodge trains')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top hit in the list should be *Subway Surfers*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Finding terms with low/high idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the inverse document frequency (idf) of a term is the lower the more documents from a given collection the term appears in. To get a better understanding for this concept, your next task is to write code to find out which terms have the lowest/highest idf with respect to the app descriptions.\n",
    "\n",
    "Start by sorting the terms in increasing order of idf, breaking ties by falling back on alphabetic order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names_out()\n",
    "idf_sums = vectorizer.idf_\n",
    "\n",
    "# Pair up terms with their idf sums. Sorted on ascending\n",
    "data = [(term, idf_sums[col]) for col, term in enumerate(terms)]\n",
    "term_value_pairs = sorted(data, key=lambda pair: (pair[1], pair[0]))\n",
    "\n",
    "terms = [pair[0] for pair in term_value_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, print the 10 terms with the lowest/highest idf. How do you explain the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10:\n",
      "['game', 'play', 'feature', 'free', 'new', 'world', 'time', 'app', 'fun', 'use']\n",
      "\n",
      "Last 10:\n",
      "['ÌöåÏõêÍ∞ÄÏûÖÏóê', 'ÌöåÏõêÏùÑ', 'ÌöçÎìùÌïú', 'Ìö®Í≥º', 'Ìö®Í≥ºÏùå', 'Ô¨Ånd', 'Ô¨Ånger', 'Ô¨Ånish', 'Ô¨Årst', 'Ô¨Çye']\n",
      "\n",
      "\n",
      "The first 10 terms, with the lowest idf values, are the most commonly occuring\n",
      "in the documents\n",
      "\n",
      "The last 10 terms, with the highest idf values, occurs in fewest documents\n",
      "(probably only in one of them). Mostly Korean(?) words because its a \n",
      "language not used much in the given context.\n"
     ]
    }
   ],
   "source": [
    "print('First 10:')\n",
    "print(terms[:10])\n",
    "\n",
    "print('\\nLast 10:')\n",
    "print(terms[-10:])\n",
    "\n",
    "print('\\n\\nThe first 10 terms, with the lowest idf values, are the most commonly occuring' + \n",
    "     '\\nin the documents')\n",
    "print('\\nThe last 10 terms, with the highest idf values, occurs in fewest documents' + \n",
    "      '\\n(probably only in one of them). Mostly Korean(?) words because its a '+\n",
    "      '\\nlanguage not used much in the given context.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Keyword extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple method for extracting salient keywords from a document is to pick the $k$ terms with the highest tf‚Äìidf value. Your last task in this lab is to implement this method. More specifically, we ask you to implement a function `keywords` that extracts keywords from a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def keywords(text, n=10):\n",
    "    term_array = np.array(vectorizer.get_feature_names_out())\n",
    "    vectorized_query = vectorizer.transform([text]).toarray().flatten()\n",
    "    \n",
    "    \n",
    "    # Debug: Printing the tf-idf values\n",
    "    pairs = [\n",
    "        (term_array[i], vectorized_query[i]) \\\n",
    "         for i in range(len(vectorized_query)) \\\n",
    "         if vectorized_query[i] > 0.0\n",
    "    ]\n",
    "    print(*sorted(pairs, key=lambda x: x[1])[-1:-n-1:-1], sep='\\n', end='\\n'*3)\n",
    "    \n",
    "    \n",
    "    sorted_index = np.argsort(vectorized_query)\n",
    "    return term_array[sorted_index[-1:-n-1:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should conform to the following specification:\n",
    "\n",
    "<strong>keywords</strong> (<em>text</em>, <em>n</em> = 10)\n",
    "\n",
    "> Returns a list with the $n$ (default value: 10) most salient keywords from the specified text, as measured by their tf‚Äìidf value relative to the collection of app descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train', 0.3668995405502609)\n",
      "('railway', 0.21071250638620947)\n",
      "('railroad', 0.21071250638620947)\n",
      "('rail', 0.19439227146214436)\n",
      "('chaos', 0.17561322826201356)\n",
      "('crash', 0.15529418865429875)\n",
      "('tram', 0.1229058922552027)\n",
      "('timetable', 0.1229058922552027)\n",
      "('railyard', 0.1229058922552027)\n",
      "('overcast', 0.1229058922552027)\n",
      "\n",
      "\n",
      "['train' 'railway' 'railroad' 'rail' 'chaos' 'crash' 'timetable' 'haul'\n",
      " 'overcast' 'locomotive']\n"
     ]
    }
   ],
   "source": [
    "print(keywords(df['description'][1428]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give the following output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "['train', 'railway', 'railroad', 'rail', 'chaos', 'crash', 'timetable', 'haul', 'overcast', 'locomotive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following reflection questions are questions that you could be asked in the oral exam. Try to answer each of them in the form of a short text and put it in the cell below. You will get feedback on your answers from your lab assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RQ 1.1:** Why do we remove common stop words and lemmatise the text? Can you give an example of a scenario where, in addition to common stopwords, there are also *domain-specific* or *application-specific* stop words?\n",
    "\n",
    "> We remove stop words and lemmatise the text to make the sparse vector representation smaller and more targeted. Stop words are grammar specific and are widely used everywhere in text, thus they will inevitably be the most frequently used words (distributed according to zipfs law). However, they provide no topical meaning in the given context and will only be overshadowing the more relevant words, unnecessarily waste memory resources, and slow down search time. \n",
    "Simularly, there are many grammatical forms for a given word, but the meaning is always more or less the same. Thus, converting all possible variations of a word to its lemma representation will remove redundancy, make the term frequency based representation faster, and more accurate.\n",
    "\n",
    "> *Domain-specific* stop words are words that will often occur in a given context, but not provide any relevant meaning to the text. The dataset for this lab is the \"app description\" dataset, and one of the most common word is \"app\". This is a typical word in this context and do not provide any useful meaning to the description and can thus be considered a *Domain-specific* stop word. Another similar example would be the word \"movie\" or \"actor\" in a dataset based on movie descriptions.\n",
    "\n",
    "\n",
    "**RQ 1.2:** In Problem&nbsp;2, what do the dimensions of the matrix `X` correspond to? This matrix gives rise to different types of *representations*. Explain what these representations are. What information from the data is preserved, what information is lost in these representations?\n",
    "\n",
    "> Every entry in the array represents a document, and every document is represented with a sparse vector containing tf_idf value for all terms from the trained vectorizer, in order. Meaning: if a term does not exist in the document, it will be represented by a zero, and if it does exist, it will be a weighted value based on the \"term frequency inverse document frequency\" value. It is meant to represent how important a word is to a document by combining the term frequency with the inverse document frequency which attempts to give more discriminative power to terms by penalizing generic words, and thus the information about frequency is partly lost due to this compensation.\n",
    "\n",
    "**RQ 1.3:** What does it mean that a term has a high/low idf value? Based on this, how can we use idf to automatically identify stop words? Why do you think is idf not used as a term weighting on its own, but always in connection with term frequency (tf‚Äìidf)?\n",
    "\n",
    "> A high idf value on a term means that it is very frequently used in the given dataset. This implies that it might not provide any unique and/or relevant meaning in the context. Usually it would indicate that the word is a stop word. It could be a standard grammatical stop word or a domain-specific stop word. As such, it is possible to automatically identify these stop words by looking at the terms with the highest idf values.\n",
    "However, sometimes terms with high idf values provides relevant meaning as well. For example: The dataset for this lab had \"game\" as the most common term (excluding the already removed grammatical stop words), but it is actually a relevant term which says that this app is a game. Not all apps are games, so it could be useful information in the given context.\n",
    "\n",
    "> Only using the idf value means that there is no distinguishing between a term that only occurs once in a document and a term that occurs many times in that document. If the document is a description of a train game, then the term 'train' will most likely occur many times, but this information will be lost and overshadowed with other terms with higher idf values, like for example, some commonly used adjectives. Since 'train' is something we would expect to find in that context, using the tf to enhance the idf values here is highly relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Enter your answers here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations on finishing L1! üëç**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
