{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L5: Information extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information extraction (IE) is the task of identifying named entities and semantic relations between these entities in text data. In this lab we will focus on two sub-tasks in IE, **named entity recognition** (identifying mentions of entities) and **entity linking** (matching these mentions to entities in a knowledge base)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we will be using has been tokenized following the conventions of the [Penn Treebank](ftp://ftp.cis.upenn.edu/pub/treebank/public_html/tokenization.html), and we need to prevent spaCy from using its own tokenizer on top of this. We therefore override spaCy&rsquo;s tokenizer with one that simply splits on space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "class WhitespaceTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return Doc(self.vocab, words=text.split(' '))\n",
    "\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main data set for this lab is a collection of news wire articles in which mentions of named entities have been annotated with page names from the [English Wikipedia](https://en.wikipedia.org/wiki/). The next code cell loads the training and the development parts of the data into Pandas data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "with bz2.open('ner-train.tsv.bz2', 'rt') as source:\n",
    "    df_train = pd.read_csv(source, sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "with bz2.open('ner-dev.tsv.bz2', 'rt') as source:\n",
    "    df_dev = pd.read_csv(source, sep='\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in these two data frames corresponds to one mention of a named entity and has five columns:\n",
    "\n",
    "1. a unique identifier for the sentence containing the entity mention\n",
    "2. the pre-tokenized sentence, with tokens separated by spaces\n",
    "3. the start position of the token span containing the entity mention\n",
    "4. the end position of the token span (exclusive, as in Python list indexing)\n",
    "5. the entity label; either a Wikipedia page name or the generic label `--NME--`\n",
    "\n",
    "The following cell prints the first five samples from the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>beg</th>\n",
       "      <th>end</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000-000</td>\n",
       "      <td>EU rejects German call to boycott British lamb .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000-000</td>\n",
       "      <td>EU rejects German call to boycott British lamb .</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000-000</td>\n",
       "      <td>EU rejects German call to boycott British lamb .</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>United_Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000-001</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000-002</td>\n",
       "      <td>BRUSSELS 1996-08-22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Brussels</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_id                                          sentence  beg  end  \\\n",
       "0    0000-000  EU rejects German call to boycott British lamb .    0    1   \n",
       "1    0000-000  EU rejects German call to boycott British lamb .    2    3   \n",
       "2    0000-000  EU rejects German call to boycott British lamb .    6    7   \n",
       "3    0000-001                                   Peter Blackburn    0    2   \n",
       "4    0000-002                               BRUSSELS 1996-08-22    0    1   \n",
       "\n",
       "            label  \n",
       "0         --NME--  \n",
       "1         Germany  \n",
       "2  United_Kingdom  \n",
       "3         --NME--  \n",
       "4        Brussels  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sample, we see that the first sentence is annotated with three entity mentions:\n",
    "\n",
    "* the span 0–1 &lsquo;EU&rsquo; is annotated as a mention but only labelled with the generic `--NME--`\n",
    "* the span 2–3 &lsquo;German&rsquo; is annotated with the page [Germany](http://en.wikipedia.org/wiki/Germany)\n",
    "* the span 6–7 &lsquo;British&rsquo; is annotated with the page [United_Kingdom](http://en.wikipedia.org/wiki/United_Kingdom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Evaluation measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To warm up, we ask you to write code to print the three measures that you will be using for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_report(gold, pred):\n",
    "    precision = [1 if p in gold else 0 for p in pred]\n",
    "    precision = sum(precision)/(len(precision))\n",
    "\n",
    "    recall = [1 if g in pred else 0 for g in gold]\n",
    "    recall = sum(recall)/(len(recall))\n",
    "\n",
    "    print('precision: {0}%'.format(round(precision*100)) )\n",
    "    print('recall: {0}%'.format(round(recall*100)) )\n",
    "    print('F1: {0}%'.format(round(200*((precision*recall)/(precision+recall)))) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your code, you can run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 60%\n",
      "recall: 100%\n",
      "F1: 75%\n"
     ]
    }
   ],
   "source": [
    "evaluation_report(set(range(3)), set(range(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give you a precision of 60%, a recall of 100%, and an F1-value of 75%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Span recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first tasks that an information extraction system has to solve is to locate and classify (mentions of) named entities, such as persons and organizations. Here we will tackle the simpler task of recognizing **spans** of tokens that contain an entity mention, without the actual entity label.\n",
    "\n",
    "The English language model in spaCy features a full-fledged [named entity recognizer](https://spacy.io/usage/linguistic-features#named-entities) that identifies a variety of entities, and can be updated with new entity types by the user. Your task in this problem is to evaluate the performance of this component when predicting entity spans in the development data.\n",
    "\n",
    "Start by implementing a generator function that yields the gold-standard spans in a given data frame.\n",
    "\n",
    "**Hint:** The Pandas method [`itertuples()`](https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.itertuples.html) is useful when iterating over the rows in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_spans(df):\n",
    "    \"\"\"Yield the gold-standard mention spans in a data frame.\n",
    "\n",
    "    Args:\n",
    "        df: A data frame.\n",
    "\n",
    "    Yields:\n",
    "        The gold-standard mention spans in the specified data frame as\n",
    "        triples consisting of the sentence id, start position, and end\n",
    "        position of each span.\n",
    "    \"\"\"\n",
    "    for row in df.itertuples():\n",
    "        yield (row[1], row[3], row[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your code, you can count the spans yielded by your function. When called on the development data, you should get a total of 5,917 unique triples. The first triple and the last triple should be\n",
    "\n",
    "    ('0946-000', 2, 3)\n",
    "    ('1161-010', 1, 3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5917\n",
      "('0946-000', 2, 3)\n",
      "('1161-010', 1, 3)\n"
     ]
    }
   ],
   "source": [
    "spans_dev_gold = list(gold_spans(df_dev))\n",
    "print(len(spans_dev_gold))\n",
    "print(spans_dev_gold[0])\n",
    "print(spans_dev_gold[5916])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to write code that calls spaCy to predict the named entities in the development data, and to evaluate the accuracy of these predictions in terms of precision, recall, and F1. Print these scores using the function that you wrote for Problem&nbsp;1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 53%\n",
      "recall: 69%\n",
      "F1: 60%\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def ne_pred(df):\n",
    "    for row in df.itertuples():\n",
    "        senid = row[1]\n",
    "        sen = row[2]\n",
    "        doc = nlp(sen)\n",
    "        for ent in doc.ents:\n",
    "            yield senid, ent.start, ent.end\n",
    "\n",
    "            res = set(ne_pred(df_dev))\n",
    "spans_dev_gold = set(gold_spans(df_dev))\n",
    "evaluation_report(spans_dev_gold,res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you were able to see in Problem&nbsp;2, the span accuracy of the named entity recognizer is far from perfect. In particular, only slightly more than half of the predicted spans are correct according to the gold standard. Your next task is to analyse this result in more detail.\n",
    "\n",
    "Here is a function that prints the false positives as well as the false negatives spans for a data frame, given a reference set of gold-standard spans and a candidate set of predicted spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def error_report(df, spans_gold, spans_pred):\n",
    "    false_pos = defaultdict(list)\n",
    "    for s, b, e in spans_pred - spans_gold:\n",
    "        false_pos[s].append((b, e))\n",
    "    false_neg = defaultdict(list)\n",
    "    for s, b, e in spans_gold - spans_pred:\n",
    "        false_neg[s].append((b, e))\n",
    "    for row in df.drop_duplicates('sentence_id').itertuples():\n",
    "        if row.sentence_id in false_pos or row.sentence_id in false_neg:\n",
    "            print('Sentence:', row.sentence)\n",
    "            for b, e in false_pos[row.sentence_id]:\n",
    "                print('  FP:', ' '.join(row.sentence.split()[b:e]))\n",
    "            for b, e in false_neg[row.sentence_id]:\n",
    "                print('  FN:', ' '.join(row.sentence.split()[b:e]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to inspect and analyse the errors that the automated prediction makes. Can you see any patterns? Base your analysis on the first 500 rows of the training data. Summarize your observations in a short text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .\n",
      "  FN: LEICESTERSHIRE\n",
      "Sentence: West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and 39 runs in two days to take over at the head of the county championship .\n",
      "  FP: four\n",
      "  FP: 39\n",
      "  FP: 38\n",
      "  FP: two days\n",
      "  FP: Friday\n",
      "  FN: Somerset\n",
      "Sentence: Their stay on top , though , may be short-lived as title rivals Essex , Derbyshire and Surrey all closed in on victory while Kent made up for lost time in their rain-affected match against Nottinghamshire .\n",
      "  FN: Nottinghamshire\n",
      "Sentence: After bowling Somerset out for 83 on the opening morning at Grace Road , Leicestershire extended their first innings by 94 runs before being bowled out for 296 with England discard Andy Caddick taking three for 83 .\n",
      "  FP: 94\n",
      "  FP: three\n",
      "  FP: 83\n",
      "  FP: 296\n",
      "  FP: the opening morning\n",
      "  FP: first\n",
      "  FP: 83\n",
      "  FN: Leicestershire\n",
      "Sentence: Trailing by 213 , Somerset got a solid start to their second innings before Simmons stepped in to bundle them out for 174 .\n",
      "  FP: 174\n",
      "  FP: 213\n",
      "  FP: second\n",
      "Sentence: Essex , however , look certain to regain their top spot after Nasser Hussain and Peter Such gave them a firm grip on their match against Yorkshire at Headingley .\n",
      "  FN: Essex\n",
      "Sentence: Hussain , considered surplus to England 's one-day requirements , struck 158 , his first championship century of the season , as Essex reached 372 and took a first innings lead of 82 .\n",
      "  FP: 372\n",
      "  FP: 158\n",
      "  FP: 82\n",
      "  FP: first championship century of the season\n",
      "  FP: one-day\n",
      "  FP: first\n",
      "Sentence: By the close Yorkshire had turned that into a 37-run advantage but off-spinner Such had scuttled their hopes , taking four for 24 in 48 balls and leaving them hanging on 119 for five and praying for rain .\n",
      "  FP: 48\n",
      "  FP: 119\n",
      "  FP: five\n",
      "  FP: four\n",
      "  FP: 24\n",
      "  FP: 37-run\n",
      "  FN: Such\n",
      "Sentence: At the Oval , Surrey captain Chris Lewis , another man dumped by England , continued to silence his critics as he followed his four for 45 on Thursday with 80 not out on Friday in the match against Warwickshire .\n",
      "  FP: Thursday\n",
      "  FP: four\n",
      "  FP: 80\n",
      "  FP: Friday\n",
      "  FN: Oval\n",
      "  FN: Warwickshire\n",
      "Sentence: He was well backed by England hopeful Mark Butcher who made 70 as Surrey closed on 429 for seven , a lead of 234 .\n",
      "  FP: 429\n",
      "  FP: 234\n",
      "  FP: 70\n",
      "  FP: seven\n",
      "Sentence: Derbyshire kept up the hunt for their first championship title since 1936 by reducing Worcestershire to 133 for five in their second innings , still 100 runs away from avoiding an innings defeat .\n",
      "  FP: second\n",
      "  FP: first\n",
      "  FP: 133\n",
      "  FP: 1936\n",
      "  FP: five\n",
      "  FP: 100\n",
      "  FN: Derbyshire\n",
      "Sentence: Australian Tom Moody took six for 82 but Chris Adams , 123 , and Tim O'Gorman , 109 , took Derbyshire to 471 and a first innings lead of 233 .\n",
      "  FP: 471\n",
      "  FP: 233\n",
      "  FP: 109\n",
      "  FP: six\n",
      "  FP: 123\n",
      "  FP: first\n",
      "  FP: 82\n",
      "Sentence: After the frustration of seeing the opening day of their match badly affected by the weather , Kent stepped up a gear to dismiss Nottinghamshire for 214 .\n",
      "  FP: 214\n",
      "  FP: the opening day\n",
      "Sentence: They were held up by a gritty 84 from Paul Johnson but ex-England fast bowler Martin McCague took four for 55 .\n",
      "  FP: 84\n",
      "  FP: 55\n",
      "  FP: four\n",
      "Sentence: By stumps Kent had reached 108 for three .\n",
      "  FP: three\n",
      "  FP: 108\n",
      "Sentence: CRICKET - ENGLISH COUNTY CHAMPIONSHIP SCORES .\n",
      "  FN: ENGLISH COUNTY CHAMPIONSHIP\n",
      "Sentence: Result and close of play scores in English county championship matches on Friday :\n",
      "  FP: Friday\n",
      "Sentence: Leicester : Leicestershire beat Somerset by an innings and 39 runs .\n",
      "  FP: 39\n",
      "  FN: Leicestershire\n",
      "  FN: Somerset\n",
      "  FN: Leicester\n",
      "Sentence: Somerset 83 and 174 ( P. Simmons 4-38 ) , Leicestershire 296 .\n",
      "  FP: 174\n",
      "  FP: 296\n",
      "  FN: Leicestershire\n",
      "  FN: P. Simmons\n",
      "Sentence: Leicestershire 22 points , Somerset 4 .\n",
      "  FP: 22\n",
      "  FN: Leicestershire\n",
      "Sentence: Chester-le-Street : Glamorgan 259 and 207 ( A. Dale 69 , H. Morris 69 ; D. Blenkiron 4-43 ) , Durham 114 ( S. Watkin 4-28 ) and 81-3 .\n",
      "  FP: 81-3\n",
      "  FP: 259\n",
      "  FP: 207\n",
      "  FP: Durham 114\n",
      "  FN: A. Dale\n",
      "  FN: Durham\n",
      "  FN: Glamorgan\n",
      "  FN: Chester-le-Street\n",
      "  FN: S. Watkin\n",
      "Sentence: Tunbridge Wells : Nottinghamshire 214 ( P. Johnson 84 ; M. McCague 4-55 ) , Kent 108-3 .\n",
      "  FP: M. McCague 4-55\n",
      "  FP: P. Johnson 84\n",
      "  FP: 214\n",
      "  FN: Nottinghamshire\n",
      "  FN: M. McCague\n",
      "  FN: P. Johnson\n",
      "  FN: Kent\n",
      "  FN: Tunbridge Wells\n",
      "Sentence: London ( The Oval ) : Warwickshire 195 , Surrey 429-7 ( C. Lewis 80 not out , M. Butcher 70 , G. Kersey 63 , J. Ratcliffe 63 , D. Bicknell 55 ) .\n",
      "  FP: 55\n",
      "  FP: 195\n",
      "  FP: J. Ratcliffe 63\n",
      "  FN: J. Ratcliffe\n",
      "  FN: C. Lewis\n",
      "  FN: Warwickshire\n",
      "  FN: Surrey\n",
      "Sentence: Hove : Sussex 363 ( W. Athey 111 , V. Drakes 52 ; I. Austin 4-37 ) , Lancashire 197-8 ( W. Hegg 54 )\n",
      "  FP: 363\n",
      "  FP: I. Austin 4-37\n",
      "  FP: 52\n",
      "  FP: 111\n",
      "  FN: Lancashire\n",
      "  FN: Sussex\n",
      "  FN: Hove\n",
      "  FN: I. Austin\n",
      "Sentence: Portsmouth : Middlesex 199 and 426 ( J. Pooley 111 , M. Ramprakash 108 , M. Gatting 83 ) , Hampshire 232 and 109-5 .\n",
      "  FP: 199\n",
      "  FP: J. Pooley 111\n",
      "  FP: 426\n",
      "  FP: 108\n",
      "  FP: 232\n",
      "  FN: Middlesex\n",
      "  FN: Portsmouth\n",
      "  FN: Hampshire\n",
      "  FN: J. Pooley\n",
      "Sentence: Chesterfield : Worcestershire 238 and 133-5 , Derbyshire 471 ( J. Adams 123 , T.O'Gorman 109 not out , K. Barnett 87 ; T. Moody 6-82 )\n",
      "  FP: K. Barnett 87\n",
      "  FP: T.O'Gorman 109\n",
      "  FP: J. Adams 123\n",
      "  FP: T. Moody 6-82\n",
      "  FP: 238\n",
      "  FN: J. Adams\n",
      "  FN: Worcestershire\n",
      "  FN: T. Moody\n",
      "  FN: K. Barnett\n",
      "  FN: Derbyshire\n",
      "  FN: T.O'Gorman\n",
      "Sentence: Bristol : Gloucestershire 183 and 185-6 ( J. Russell 56 not out ) , Northamptonshire 190 ( K. Curran 52 ; A. Smith 5-68 ) .\n",
      "  FP: Northamptonshire 190\n",
      "  FP: 183\n",
      "  FP: J. Russell 56\n",
      "  FP: A. Smith 5-68\n",
      "  FN: Bristol\n",
      "  FN: J. Russell\n",
      "  FN: K. Curran\n",
      "  FN: Gloucestershire\n",
      "  FN: A. Smith\n",
      "  FN: Northamptonshire\n",
      "Sentence: CRICKET - 1997 ASHES INTINERARY .\n",
      "  FP: 1997\n",
      "  FN: ASHES\n",
      "Sentence: Australia will defend the Ashes in\n",
      "  FN: Ashes\n",
      "Sentence: starting on May 13 next year , the Test and County Cricket Board\n",
      "  FP: May 13 next year\n",
      "  FP: County Cricket Board\n",
      "  FP: Test\n",
      "  FN: Test and County Cricket Board\n",
      "Sentence: Australia will also play three one-day internationals and\n",
      "  FP: three\n",
      "Sentence: English county sides and another against British Universities ,\n",
      "  FP: British\n",
      "  FN: British Universities\n",
      "Sentence: as well as one-day matches against the Minor Counties and\n",
      "  FN: Minor Counties\n",
      "Sentence: May 13 Arrive in London\n",
      "  FP: May 13\n",
      "Sentence: May 14 Practice at Lord 's\n",
      "  FP: May 14 Practice\n",
      "  FN: Lord 's\n",
      "Sentence: May 15 v Duke of Norfolk 's XI ( at Arundel )\n",
      "  FP: May 15\n",
      "  FN: Arundel\n",
      "  FN: Duke of Norfolk 's XI\n",
      "Sentence: May 17 v Northampton\n",
      "  FP: May 17\n",
      "Sentence: May 18 v Worcestershire\n",
      "  FP: May 18\n",
      "Sentence: May 20 v Durham\n",
      "  FP: May 20\n",
      "  FN: Durham\n",
      "Sentence: May 22 First one-day international ( at Headingley ,\n",
      "  FP: May 22 First\n",
      "Sentence: Leeds )\n",
      "  FN: Leeds\n",
      "Sentence: May 24 Second one-day international ( at The Oval ,\n",
      "  FP: one-day\n",
      "  FP: May 24 Second\n",
      "  FN: The Oval\n",
      "Sentence: May 25 Third one-day international ( at Lord 's , London )\n",
      "  FP: May 25 Third\n",
      "  FP: one-day\n",
      "  FN: Lord 's\n",
      "Sentence: May 27-29 v Gloucestershire or Sussex or Surrey ( three\n",
      "  FP: May 27-29\n",
      "  FP: three\n",
      "  FN: Gloucestershire\n",
      "  FN: Sussex\n",
      "Sentence: May 31 - June 2 v Derbyshire ( three days )\n",
      "  FP: three days\n",
      "  FP: May 31 - June 2\n",
      "  FN: Derbyshire\n",
      "Sentence: June 5-9 First test match ( at Edgbaston , Birmingham )\n",
      "  FP: First\n",
      "  FP: June 5-9\n",
      "Sentence: June 14-16 v Leicestershire ( three days )\n",
      "  FP: June 14-16\n",
      "  FP: three days\n",
      "  FN: Leicestershire\n",
      "Sentence: June 19-23 Second test ( at Lord 's )\n",
      "  FP: June 19-23\n",
      "  FP: Second\n",
      "  FN: Lord 's\n",
      "Sentence: June 25-27 v British Universities ( at Oxford , three days )\n",
      "  FP: June 25-27\n",
      "  FP: three days\n",
      "  FP: British\n",
      "  FN: British Universities\n",
      "Sentence: June 28-30 v Hampshire ( three days )\n",
      "  FP: three days\n",
      "  FP: June 28-30\n",
      "  FN: Hampshire\n",
      "Sentence: July 3-7 Third test ( at Old Trafford , Manchester )\n",
      "  FP: Trafford\n",
      "  FP: July 3-7 Third\n",
      "  FN: Old Trafford\n",
      "Sentence: July 9 v Minor Counties XI\n",
      "  FP: July 9\n",
      "  FN: Minor Counties XI\n",
      "Sentence: July 12 v Scotland\n",
      "  FP: July 12\n",
      "Sentence: July 16-18 v Glamorgan ( three days )\n",
      "  FP: three days\n",
      "  FP: July 16-18\n",
      "Sentence: July 19-21 v Middlesex ( three days )\n",
      "  FP: July 19-21\n",
      "  FP: three days\n",
      "  FN: Middlesex\n",
      "Sentence: July 24-28 Fourth test ( at Headingley )\n",
      "  FP: 24-28 Fourth\n",
      "  FP: July\n",
      "Sentence: August 1-4 v Somerset ( four days )\n",
      "  FP: four days\n",
      "  FP: August 1-4\n",
      "Sentence: August 7-11 Fifth test ( at Trent Bridge , Nottingham )\n",
      "  FP: Fifth\n",
      "  FP: August 7-11\n",
      "Sentence: August 16-18 v Kent ( three days )\n",
      "  FP: three days\n",
      "  FP: August 16-18\n",
      "Sentence: August 21-25 Sixth test ( at The Oval , London ) .\n",
      "  FP: August\n",
      "  FP: Sixth\n",
      "  FN: The Oval\n",
      "Sentence: SOCCER - SHEARER NAMED AS ENGLAND CAPTAIN .\n",
      "  FP: ENGLAND CAPTAIN\n",
      "  FN: SHEARER\n",
      "  FN: ENGLAND\n",
      "Sentence: The world 's costliest footballer Alan Shearer was named as the new England captain on Friday .\n",
      "  FP: Friday\n",
      "Sentence: The 26-year-old , who joined Newcastle for 15 million pounds sterling ( $ 23.4 million ) , takes over from Tony Adams , who led the side during the European championship in June , and former captain David Platt .\n",
      "  FP: $ 23.4 million\n",
      "  FP: 15 million pounds\n",
      "  FP: June\n",
      "Sentence: Adams and Platt are both injured and will miss England 's opening World Cup qualifier against Moldova on Sunday .\n",
      "  FP: Sunday\n",
      "  FN: Adams\n",
      "Sentence: Shearer takes the captaincy on a trial basis , but new coach Glenn Hoddle said he saw no reason why the former Blackburn and Southampton skipper should not make the post his own .\n",
      "  FN: Shearer\n",
      "Sentence: \" I 'm sure there wo n't be a problem , I 'm sure Alan is the man for the job , \" Hoddle said .\n",
      "  FN: Hoddle\n",
      "Sentence: \" There were three or four people who could have done it but when I spoke to Alan he was up for it and really wanted it .\n",
      "  FP: three\n",
      "  FP: four\n",
      "Sentence: Shearer 's Euro 96 striking partner Teddy Sheringham withdrew from the squad with an injury on Friday .\n",
      "  FP: Friday\n",
      "Sentence: BELGRADE 1996-08-30\n",
      "  FP: 1996-08-30\n",
      "  FN: BELGRADE\n",
      "Sentence: Red Star ( Yugoslavia ) beat Dinamo ( Russia ) 92-90 ( halftime\n",
      "  FP: 92-90\n",
      "Sentence: SOCCER - ROMANIA BEAT LITHUANIA IN UNDER-21 MATCH .\n",
      "  FP: SOCCER - ROMANIA BEAT LITHUANIA\n",
      "  FN: ROMANIA\n",
      "  FN: LITHUANIA\n",
      "Sentence: BUCHAREST 1996-08-30\n",
      "  FP: BUCHAREST 1996-08-30\n",
      "  FN: BUCHAREST\n",
      "Sentence: Romania beat Lithuania 2-1 ( halftime 1-1 ) in their European under-21 soccer match on Friday .\n",
      "  FP: Friday\n",
      "Sentence: Romania - Cosmin Contra ( 31st ) , Mihai Tararache ( 75th )\n",
      "  FP: Romania - Cosmin Contra (\n",
      "  FP: 75th\n",
      "  FP: 31st\n",
      "  FN: Romania\n",
      "  FN: Cosmin Contra\n",
      "Sentence: Lithuania - Danius Gleveckas ( 13rd )\n",
      "  FP: 13rd\n",
      "  FP: Lithuania - Danius Gleveckas\n",
      "  FN: Danius Gleveckas\n",
      "  FN: Lithuania\n",
      "Sentence: SOCCER - ROTOR FANS LOCKED OUT AFTER VOLGOGRAD VIOLENCE .\n",
      "  FN: VOLGOGRAD\n",
      "  FN: ROTOR\n",
      "Sentence: MOSCOW 1996-08-30\n",
      "  FP: 1996-08-30\n",
      "Sentence: Rotor Volgograd must play their next home game behind closed doors after fans hurled bottles and stones at Dynamo Moscow players during a 1-0 home defeat on Saturday that ended Rotor 's brief spell as league leaders .\n",
      "  FP: Saturday\n",
      "Sentence: The head of the Russian league 's disciplinary committee , Anatoly Gorokhovsky , said on Friday that Rotor would play Lada Togliatti to empty stands on September 3 .\n",
      "  FP: September 3\n",
      "  FP: Russian league 's\n",
      "  FP: Friday\n",
      "  FN: Russian\n",
      "Sentence: The club , who put Manchester United out of last year 's UEFA Cup , were fined $ 1,000 .\n",
      "  FP: 1,000\n",
      "  FP: last year 's\n",
      "Sentence: Despite the defeat , Rotor are well placed with 11 games to play in the championship .\n",
      "  FP: 11\n",
      "Sentence: Lying three points behind Alania and two behind Dynamo Moscow , the Volgograd side have a game in hand over the leaders and two over the Moscow club .\n",
      "  FP: two\n",
      "  FP: three\n",
      "  FP: two\n",
      "Sentence: BOXING - PANAMA 'S ROBERTO DURAN FIGHTS THE SANDS OF TIME .\n",
      "  FP: BOXING - PANAMA 'S\n",
      "  FN: PANAMA\n",
      "  FN: ROBERTO DURAN\n",
      "Sentence: PANAMA CITY 1996-08-30\n",
      "  FP: 1996-08-30\n",
      "Sentence: Panamanian boxing legend Roberto \" Hands of Stone \" Duran climbs into the ring on Saturday in another age-defying attempt to sustain his long career .\n",
      "  FP: Duran\n",
      "  FP: Saturday\n",
      "  FP: Roberto \" Hands of Stone \"\n",
      "  FN: Roberto \" Hands of Stone \" Duran\n",
      "Sentence: Duran , 45 , takes on little-known Mexican Ariel Cruz , 30 , in a super middleweight non-title bout in Panama City .\n",
      "  FP: Mexican Ariel Cruz\n",
      "  FP: 30\n",
      "  FP: 45\n",
      "  FN: Ariel Cruz\n",
      "  FN: Mexican\n",
      "Sentence: The fight , Duran 's first on home soil for 10 years , is being billed here as the \" Return of the Legend \" and Duran still talks as if he was in his prime .\n",
      "  FP: first\n",
      "  FP: 10 years\n",
      "  FP: Legend\n",
      "  FN: Return of the Legend\n",
      "Sentence: If he loses Saturday , it could devalue his position as one of the world 's great boxers , \" Panamanian Boxing Association President Ramon Manzanares said .\n",
      "  FP: Saturday\n",
      "  FP: Panamanian Boxing Association\n",
      "  FN: Panamanian\n",
      "  FN: Boxing Association\n",
      "Sentence: Duran , whose 97-12 record spans three decades , hopes a win in the 10-round bout will earn him a rematch against Puerto Rico 's Hector \" Macho \" Camacho .\n",
      "  FP: 97-12\n",
      "  FP: three decades\n",
      "  FP: Puerto Rico 's\n",
      "  FN: Hector \" Macho \" Camacho\n",
      "  FN: Puerto Rico\n",
      "Sentence: Camacho took a controversial points decision against the Panamanian in Atlantic City in June in a title fight .\n",
      "  FP: June\n",
      "Sentence: SQUASH - HONG KONG OPEN QUARTER-FINAL RESULTS .\n",
      "  FN: HONG KONG OPEN\n",
      "Sentence: HONG KONG 1996-08-30\n",
      "  FP: 1996-08-30\n",
      "Sentence: Quarter-final results in the Hong Kong Open on Friday ( prefix number denotes seeding ) : 1 - Jansher Khan ( Pakistan ) beat Mark Cairns ( England ) 15-10 15-6 15-7\n",
      "  FP: Friday\n",
      "  FP: 15-10\n",
      "  FP: 15-7\n",
      "  FP: the Hong Kong\n",
      "  FN: Jansher Khan\n",
      "  FN: Hong Kong Open\n",
      "Sentence: 4 - Peter Nicol ( Scotland ) beat 7 - Chris Walker ( England ) 15-8 15-13 13-15 15-9\n",
      "  FP: 15-13\n",
      "  FN: Peter Nicol\n",
      "  FN: Chris Walker\n",
      "  FN: England\n",
      "Sentence: 2 - Rodney Eyles ( Australia ) beat Derek Ryan ( Ireland ) 15-6 15-9 11-15 15-10 .\n",
      "  FN: Rodney Eyles\n",
      "Sentence: SEOUL 1996-08-30\n",
      "  FP: SEOUL 1996-08-30\n",
      "  FN: SEOUL\n",
      "Sentence: Pohang 3 Ulsan 2 ( halftime 1-0 )\n",
      "  FP: Pohang 3\n",
      "  FN: Pohang\n",
      "  FN: Ulsan\n",
      "Sentence: Puchon 2 Chonbuk 1 ( halftime 1-1 )\n",
      "  FP: 2\n",
      "  FN: Puchon\n",
      "  FN: Chonbuk\n",
      "Sentence: Puchon 3 1 0 6 1 10\n",
      "  FP: 3 1\n",
      "  FN: Puchon\n",
      "Sentence: Chonan 3 0 1 13 10 9\n",
      "  FP: 3\n",
      "  FN: Chonan\n",
      "Sentence: Pohang 2 1 1 11 10 7\n",
      "  FP: Pohang 2 1 1 11 10 7\n",
      "  FN: Pohang\n",
      "Sentence: Suwan 1 3 0 7 3 6\n",
      "  FP: Suwan 1 3 0 7 3 6\n",
      "  FN: Suwan\n",
      "Sentence: Ulsan 1 0 2 8 9 3\n",
      "  FP: 1\n",
      "Sentence: Anyang 0 3 1 6 9 3\n",
      "  FN: Anyang\n",
      "Sentence: Chonnam 0 2 1 4 5 2\n",
      "  FN: Chonnam\n",
      "Sentence: Pusan 0 2 1 3 7 2\n",
      "  FN: Pusan\n",
      "Sentence: Chonbuk 0 0 3 3 7 0\n",
      "  FN: Chonbuk\n",
      "Sentence: BASEBALL - RESULTS OF S. KOREAN PROFESSIONAL GAMES .\n",
      "  FN: S. KOREAN\n",
      "Sentence: SEOUL 1996-08-30\n",
      "  FP: SEOUL 1996-08-30\n",
      "  FN: SEOUL\n",
      "Sentence: LG 2 OB 0\n",
      "  FP: LG 2 OB 0\n",
      "  FN: OB\n",
      "  FN: LG\n",
      "Sentence: Lotte 6 Hyundai 2\n",
      "  FP: 2\n",
      "  FP: 6\n",
      "Sentence: Hyundai 6 Lotte 5\n",
      "  FP: 6\n",
      "Sentence: Haitai 2 Samsung 0\n",
      "  FP: 2\n",
      "  FN: Haitai\n",
      "Sentence: Samsung 10 Haitai 3\n",
      "  FP: 10 Haitai 3\n",
      "  FN: Haitai\n",
      "Sentence: Hanwha 6 Ssangbangwool 5\n",
      "  FP: 6\n",
      "  FN: Ssangbangwool\n",
      "Sentence: Note - Lotte and Hyundai , Haitai and Samsung played two games .\n",
      "  FP: two\n",
      "  FN: Lotte\n",
      "Sentence: Haitai 64 2 43 .596 -\n",
      "  FN: Haitai\n",
      "Sentence: Ssangbangwool 59 2 49 .545 5 1/2\n",
      "  FP: 5 1/2\n",
      "  FN: Ssangbangwool\n",
      "Sentence: Hanwha 58 1 49 .542 6\n",
      "  FP: 58\n",
      "  FP: 6\n",
      "Sentence: Hyundai 57 5 49 .536 6 1/2\n",
      "  FP: 57 5 49 .536 6 1/2\n",
      "Sentence: Samsung 49 5 56 .468 14\n",
      "  FP: 49 5\n",
      "  FP: 14\n",
      "Sentence: Lotte 46 6 54 .462 14 1/2\n",
      "  FP: 46 6 54\n",
      "  FP: 14 1/2\n",
      "Sentence: LG 46 5 59 .441 17\n",
      "  FP: 17\n",
      "  FP: 46 5 59\n",
      "  FN: LG\n",
      "Sentence: OB 42 6 62 .409 20 1/2\n",
      "  FP: 20 1/2\n",
      "Sentence: TENNIS - FRIDAY 'S RESULTS FROM THE U.S. OPEN .\n",
      "  FP: U.S.\n",
      "  FN: U.S. OPEN\n",
      "Sentence: Results from the U.S. Open Tennis Championships at the National Tennis Centre on Friday ( prefix number denotes seeding ) :\n",
      "  FP: Friday\n",
      "  FP: the National Tennis Centre\n",
      "  FN: National Tennis Centre\n",
      "  FN: U.S. Open Tennis Championships\n",
      "Sentence: Sandrine Testud ( France ) beat Ines Gorrochategui ( Argentina ) 4-6 6-2 6-1\n",
      "  FP: 6-1\n",
      "Sentence: 4 - Goran Ivanisevic ( Croatia ) beat Scott Draper ( Australia ) 6-7 ( 1-7 ) 6-3 6-4 6-4\n",
      "  FP: 4\n",
      "  FP: 1-7\n",
      "  FN: Australia\n",
      "Sentence: Mark Philippoussis ( Australia ) beat Andrei Olhovskiy ( Russia ) 6 - 3 6-4 6-2\n",
      "  FP: 6 - 3\n",
      "Sentence: Sjeng Schalken ( Netherlands ) beat David Rikl ( Czech Republic ) 6 - 2 6-4 6-4\n",
      "  FP: 6 - 2\n",
      "Sentence: Guy Forget ( France ) beat 17 - Felix Mantilla ( Spain ) 6-4 7-5 6-3\n",
      "  FP: 17\n",
      "  FN: Felix Mantilla\n",
      "Sentence: Alexander Volkov ( Russia ) beat Mikael Tillstrom ( Sweden ) 1-6 6- 4 6-1 4-6 7-6 ( 10-8 )\n",
      "  FP: 4\n",
      "  FP: 10-8\n",
      "Sentence: Jonas Bjorkman ( Sweden ) beat David Nainkin ( South Africa ) ) 6-4 6-1 6-1\n",
      "  FP: 6-4 6-1\n",
      "Sentence: 8 - Lindsay Davenport ( U.S. ) beat Anne-Gaelle Sidot ( France ) 6-0 6-3\n",
      "  FP: Davenport\n",
      "  FP: Sidot\n",
      "  FN: Anne-Gaelle Sidot\n",
      "  FN: Lindsay Davenport\n",
      "Sentence: 4 - Conchita Martinez ( Spain ) beat Helena Sukova ( Czech Republic ) 6-4 6-3\n",
      "  FN: Conchita Martinez\n",
      "Sentence: Add Men 's singles , second round 16 - Cedric Pioline ( France ) beat Roberto Carretero ( Spain ) 4-6 6 - 2 6-2 6-1 Alex Corretja ( Spain ) beat Filippo Veglio ( Switzerland ) 6-7 ( 4- 7 ) 6-4 6-4 6-0\n",
      "  FP: second\n",
      "  FP: 16\n",
      "  FP: 7\n",
      "  FP: 4-6\n",
      "  FN: Cedric Pioline\n",
      "Sentence: Add Women 's singles , third round Linda Wild ( U.S. ) beat Barbara Rittner ( Germany ) 6-4 4-6 7-5 Asa Carlsson ( Sweden ) beat 15 - Gabriela Sabatini ( Argentina ) 7-5 3-6 6-2\n",
      "  FP: third\n",
      "  FP: 15\n",
      "  FP: 6-4 4-6 7-5 Asa Carlsson\n",
      "  FN: Asa Carlsson\n",
      "  FN: Gabriela Sabatini\n",
      "Sentence: Add Men 's singles , second round 1 - Pete Sampras ( U.S. ) beat Jiri Novak ( Czech Republic ) 6-3 1-6 6-3 4-6 6-4 Paul Haarhuis ( Netherlands ) beat Michael Tebbutt ( Australia ) 1- 6 6-2 6-2 6-3\n",
      "  FP: 6-2\n",
      "  FP: second\n",
      "  FP: 6\n",
      "  FN: Paul Haarhuis\n",
      "  FN: Pete Sampras\n",
      "Sentence: Add Women 's singles , third round Lisa Raymond ( U.S. ) beat Kimberly Po ( U.S. ) 6-3 6-2\n",
      "  FP: third\n",
      "Sentence: 2 - Monica Seles ( U.S. ) beat Dally Randriantefy ( Madagascar )\n",
      "  FP: 2 - Monica Seles\n",
      "  FN: Monica Seles\n",
      "  FN: Madagascar\n",
      "Sentence: Add men 's singles , second round 12 - Todd Martin ( U.S. ) beat Andrea Gaudenzi ( Italy ) 6-3 6-2 6-2 Stefan Edberg ( Sweden ) beat Bernd Karbacher ( Germany ) 3-6 6-3 6-3 1-0 retired ( leg injury )\n",
      "  FP: second\n",
      "  FP: 12\n",
      "Sentence: BASEBALL - MAJOR LEAGUE STANDINGS AFTER THURSDAY 'S GAMES .\n",
      "  FP: BASEBALL - MAJOR LEAGUE STANDINGS AFTER THURSDAY 'S\n",
      "  FN: MAJOR LEAGUE\n",
      "Sentence: AMERICAN LEAGUE EASTERN DIVISION\n",
      "  FP: AMERICAN LEAGUE EASTERN\n",
      "  FN: AMERICAN LEAGUE EASTERN DIVISION\n",
      "Sentence: NEW YORK 74 59 .556 -\n",
      "  FP: 74 59\n",
      "  FN: NEW YORK\n",
      "Sentence: BALTIMORE 70 63 .526 4\n",
      "  FP: 70 63\n",
      "Sentence: BOSTON 69 65 .515 5 1/2\n",
      "  FP: 5 1/2\n",
      "  FP: 69 65\n",
      "Sentence: TORONTO 63 71 .470 11 1/2\n",
      "  FP: 11 1/2\n",
      "  FP: 63 71\n",
      "  FN: TORONTO\n",
      "Sentence: DETROIT 48 86 .358 26 1/2\n",
      "  FP: 26 1/2\n",
      "  FP: 48 86\n",
      "Sentence: CENTRAL DIVISION\n",
      "  FP: CENTRAL\n",
      "  FN: CENTRAL DIVISION\n",
      "Sentence: CLEVELAND 80 53 .602 -\n",
      "  FP: 53\n",
      "  FN: CLEVELAND\n",
      "Sentence: CHICAGO 71 64 .526 10\n",
      "  FP: 10\n",
      "  FP: 71 64\n",
      "Sentence: MINNESOTA 67 67 .500 13 1/2\n",
      "  FP: 67 67\n",
      "  FP: 13 1/2\n",
      "Sentence: MILWAUKEE 64 71 .474 17\n",
      "  FP: 17\n",
      "  FN: MILWAUKEE\n",
      "Sentence: KANSAS CITY 61 74 .452 20\n",
      "  FP: 20\n",
      "  FP: 61 74\n",
      "Sentence: WESTERN DIVISION\n",
      "  FN: WESTERN DIVISION\n",
      "Sentence: SEATTLE 70 63 .526 5\n",
      "  FP: 70 63\n",
      "Sentence: OAKLAND 64 72 .471 12 1/2\n",
      "  FP: 12 1/2\n",
      "Sentence: CALIFORNIA 62 72 .463 13 1/2\n",
      "  FP: 62 72 .463\n",
      "  FP: 13 1/2\n",
      "Sentence: CLEVELAND AT TEXAS\n",
      "  FN: CLEVELAND\n",
      "Sentence: NATIONAL LEAGUE EASTERN DIVISION\n",
      "  FP: NATIONAL LEAGUE EASTERN\n",
      "  FN: NATIONAL LEAGUE EASTERN DIVISION\n",
      "Sentence: ATLANTA 83 49 .629 -\n",
      "  FP: 83 49\n",
      "Sentence: MONTREAL 71 61 .538 12\n",
      "  FP: 12\n",
      "  FP: 71\n",
      "  FN: MONTREAL\n",
      "Sentence: FLORIDA 64 70 .478 20\n",
      "  FP: 64 70\n",
      "  FP: 20\n",
      "Sentence: NEW YORK 59 75 .440 25\n",
      "  FP: 59 75 .440\n",
      "  FP: 25\n",
      "  FN: NEW YORK\n",
      "Sentence: PHILADELPHIA 54 80 .403 30\n",
      "  FP: 54 80\n",
      "  FP: 30\n",
      "Sentence: CENTRAL DIVISION\n",
      "  FP: CENTRAL\n",
      "  FN: CENTRAL DIVISION\n",
      "Sentence: HOUSTON 72 63 .533 -\n",
      "  FP: 72 63\n",
      "Sentence: ST LOUIS 69 65 .515 2 1/2\n",
      "  FP: 2 1/2\n",
      "  FP: 69 65 .515\n",
      "  FP: ST\n",
      "  FN: ST LOUIS\n",
      "Sentence: CINCINNATI 66 67 .496 5\n",
      "  FP: 66 67 .496\n",
      "  FN: CINCINNATI\n",
      "Sentence: PITTSBURGH 56 77 .421 15\n",
      "  FP: 15\n",
      "  FP: 56 77\n",
      "  FN: PITTSBURGH\n",
      "Sentence: WESTERN DIVISION\n",
      "  FN: WESTERN DIVISION\n",
      "Sentence: SAN DIEGO 75 60 .556 -\n",
      "  FP: 75 60\n",
      "  FN: SAN DIEGO\n",
      "Sentence: LOS ANGELES 72 61 .541 2\n",
      "  FP: 72 61\n",
      "  FP: 2\n",
      "Sentence: COLORADO 70 65 .519 5\n",
      "  FP: 5\n",
      "Sentence: SAN DIEGO AT MONTREAL\n",
      "  FN: SAN DIEGO\n",
      "Sentence: SAN FRANCISCO AT NEW YORK\n",
      "  FN: SAN FRANCISCO\n",
      "Sentence: COLORADO AT ST LOUIS\n",
      "  FP: ST\n",
      "  FN: ST LOUIS\n",
      "Sentence: BASEBALL - MAJOR LEAGUE RESULTS THURSDAY .\n",
      "  FP: BASEBALL - MAJOR LEAGUE RESULTS THURSDAY\n",
      "  FN: MAJOR LEAGUE\n",
      "Sentence: DETROIT 4 Kansas City 1\n",
      "  FP: 4\n",
      "Sentence: Minnesota 6 MILWAUKEE 1\n",
      "  FP: 6\n",
      "  FN: MILWAUKEE\n",
      "Sentence: CALIFORNIA 14 New York 3\n",
      "  FP: 3\n",
      "  FP: 14\n",
      "Sentence: SEATTLE 9 Baltimore 6\n",
      "  FP: 6\n",
      "  FP: 9\n",
      "  FN: Baltimore\n",
      "Sentence: San Diego 3 NEW YORK 2\n",
      "  FP: 3\n",
      "  FN: NEW YORK\n",
      "Sentence: Chicago 4 HOUSTON 3\n",
      "  FP: 3\n",
      "  FN: HOUSTON\n",
      "Sentence: Cincinnati 18 COLORADO 7\n",
      "  FP: 18\n",
      "  FN: COLORADO\n",
      "Sentence: Atlanta 5 PITTSBURGH 1\n",
      "  FP: 5\n",
      "  FP: 1\n",
      "  FN: PITTSBURGH\n",
      "Sentence: Los Angeles 2 MONTREAL 1\n",
      "  FP: 1\n",
      "  FP: Los Angeles 2\n",
      "  FN: Los Angeles\n",
      "  FN: MONTREAL\n",
      "Sentence: Florida 10 ST LOUIS 9\n",
      "  FP: 10\n",
      "Sentence: TENNIS - TARANGO , O'BRIEN SPRING TWIN UPSETS UNDER THE LIGHTS .\n",
      "  FP: SPRING TWIN\n",
      "  FN: TARANGO\n",
      "  FN: O'BRIEN\n",
      "Sentence: Andre Agassi escaped disaster on Thursday but Wimbledon finalist MaliVai Washington and Marcelo Rios were not so fortunate on a night of upsets at the U.S. Open .\n",
      "  FP: Thursday\n",
      "  FP: the U.S. Open\n",
      "  FP: a night\n",
      "  FN: U.S. Open\n",
      "Sentence: The 11th-seeded Washington fell short of reprising his Wimbledon miracle comeback as he lost to red-hot wildcard Alex O'Brien 6-3 6-4 5-7 3-6 6-3 in a two hour 51 minute struggle on the Stadium court .\n",
      "  FP: 11th-seeded\n",
      "  FP: Stadium\n",
      "  FP: Wimbledon\n",
      "  FP: 51 minute\n",
      "  FP: Alex O'Brien\n",
      "  FP: 6-4 5-7 3-6 6-3\n",
      "  FP: two hour\n"
     ]
    }
   ],
   "source": [
    "df_500 = df_dev[0:500]\n",
    "res = set(ne_pred(df_500))\n",
    "spans_dev_gold = set(gold_spans(df_500))\n",
    "error_report(df_dev,spans_dev_gold,res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Write a short text that summarises the errors that you observed*\n",
    "Many of the errors have to do with either numbers or dates, which should be easy to filter out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the insights from your error analysis to improve the automated prediction that you implemented in Problem&nbsp;2. While the best way to do this would be to [update spaCy&rsquo;s NER model](https://spacy.io/usage/linguistic-features#updating) using domain-specific training data, for this lab it suffices to write code to post-process the output produced by spaCy. You should be able to improve the F1 score from Problem&nbsp;2 by at last 15 percentage points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 81%\n",
      "recall: 64%\n",
      "F1: 72%\n"
     ]
    }
   ],
   "source": [
    "def filtered_pred(df):\n",
    "    for row in df.itertuples():\n",
    "        senid = row[1]\n",
    "        sen = row[2]\n",
    "        doc = nlp(sen)\n",
    "        for ent in doc.ents:\n",
    "            if(ent.label_!=\"DATE\" and ent.label_!=\"MONEY\" and ent.label_!=\"QUANTITY\" and ent.label_!=\"CARDINAL\" and ent.label_!=\"ORDINAL\"):\n",
    "                yield senid, ent.start, ent.end\n",
    "df_500 = df_dev[0:500]\n",
    "res = set(filtered_pred(df_500))\n",
    "spans_dev_gold = set(gold_spans(df_500))\n",
    "#error_report(df_dev,spans_dev_gold,res)\n",
    "evaluation_report(spans_dev_gold,res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that you achieve the performance goal by reporting the evaluation measures that you implemented in Problem&nbsp;1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going on, we ask you to store the outputs of the improved named entity recognizer on the development data in a new data frame. This new frame should have the same layout as the original data frame for the development data that you loaded above, but should contain the *predicted* start and end positions for each token span, rather than the gold positions. As the `label` of each span, you can use the special value `--NME--`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>beg</th>\n",
       "      <th>end</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0957-020</td>\n",
       "      <td>Add Men 's singles , second round 1 - Pete Sam...</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0949-004</td>\n",
       "      <td>Adams and Platt are both injured and will miss...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0946-012</td>\n",
       "      <td>Australian Tom Moody took six for 82 but Chris...</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0954-002</td>\n",
       "      <td>Quarter-final results in the Hong Kong Open on...</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0946-003</td>\n",
       "      <td>Their stay on top , though , may be short-live...</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_id                                           sentence  beg  end  \\\n",
       "0    0957-020  Add Men 's singles , second round 1 - Pete Sam...   12   13   \n",
       "1    0949-004  Adams and Platt are both injured and will miss...    9   10   \n",
       "2    0946-012  Australian Tom Moody took six for 82 but Chris...   20   21   \n",
       "3    0954-002  Quarter-final results in the Hong Kong Open on...   27   28   \n",
       "4    0946-003  Their stay on top , though , may be short-live...   17   18   \n",
       "\n",
       "     label  \n",
       "0  --NME--  \n",
       "1  --NME--  \n",
       "2  --NME--  \n",
       "3  --NME--  \n",
       "4  --NME--  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restemp = []\n",
    "col = [\"sentence_id\", \"sentence\", \"beg\", \"end\", \"label\"]\n",
    "for i in res:\n",
    "    senid, beg, end = i\n",
    "    label = \"--NME--\"\n",
    "    sen = df_dev[df_dev[\"sentence_id\"]==senid].iloc[0][\"sentence\"]\n",
    "    restemp.append((senid, sen, beg, end, label))\n",
    "\n",
    "res_df = pd.DataFrame(restemp, columns=col)\n",
    "res_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Entity linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a method for predicting mention spans, we turn to the task of **entity linking**, which amounts to predicting the knowledge base entity that is referenced by a given mention. In our case, for each span we want to predict the Wikipedia page that this mention references.\n",
    "\n",
    "Start by extending the generator function that you implemented in Problem&nbsp;2 to labelled spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_mentions(df):\n",
    "    \"\"\"Yield the gold-standard mentions in a data frame.\n",
    "\n",
    "    Args:\n",
    "        df: A data frame.\n",
    "\n",
    "    Yields:\n",
    "        The gold-standard mention spans in the specified data frame as\n",
    "        quadruples consisting of the sentence id, start position, end\n",
    "        position and entity label of each span.\n",
    "    \"\"\"\n",
    "    for row in df.itertuples():\n",
    "        yield (row[1], row[3], row[4], row[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive baseline for entity linking on our data set is to link each mention span to the Wikipedia page name that we get when we join the tokens in the span by underscores, as is standard in Wikipedia page names. Suppose, for example, that a span contains the two tokens\n",
    "\n",
    "    Jimi Hendrix\n",
    "\n",
    "The baseline Wikipedia page name for this span would be\n",
    "\n",
    "    Jimi_Hendrix\n",
    "\n",
    "Implement this naive baseline and evaluate its performance. Print the evaluation measures that you implemented in Problem&nbsp;1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    Here and in the remainder of this lab, you should base your entity predictions on the predicted spans that you computed in Problem&nbsp;3.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 28%\n",
      "recall: 22%\n",
      "F1: 25%\n"
     ]
    }
   ],
   "source": [
    "def baseline(df):\n",
    "    for row in df.itertuples():\n",
    "        split = row[2].split(\" \")[row[3]:row[4]]\n",
    "        joined = \"_\".join(split)\n",
    "        yield (row[1], row[3], row[4], joined)\n",
    "\n",
    "baseline_data = set(baseline(res_df))\n",
    "gold_new = set(gold_mentions(df_500))\n",
    "evaluation_report(gold_new,baseline_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Extending the training data using the knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State-of-the-art approaches to entity linking exploit information in knowledge bases. In our case, where Wikipedia is the knowledge base, one particularly useful type of information are links to other Wikipedia pages. In particular, we can interpret the anchor texts (the highlighted texts that you click on) as mentions of the entities (pages) that they link to. This allows us to harvest long lists of mention–entity pairings.\n",
    "\n",
    "The following cell loads a data frame summarizing anchor texts and page references harvested from the first paragraphs of the English Wikipedia. The data frame also contains all entity mentions in the training data (but not the development or the test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open('kb.tsv.bz2', 'rt') as source:\n",
    "    df_kb = pd.read_csv(source, sep='\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what information is available in this data, the following cell shows the entry for the anchor text `Sweden`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>entity</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17436</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>0.985768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17437</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden_national_football_team</td>\n",
       "      <td>0.014173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17438</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden_men's_national_ice_hockey_team</td>\n",
       "      <td>0.000059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mention                                 entity      prob\n",
       "17436  Sweden                                 Sweden  0.985768\n",
       "17437  Sweden          Sweden_national_football_team  0.014173\n",
       "17438  Sweden  Sweden_men's_national_ice_hockey_team  0.000059"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kb.loc[df_kb.mention == 'Sweden']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each row of the data frame contains a pair $(m, e)$ of a mention $m$ and an entity $e$, as well as the conditional probability $P(e|m)$ for mention $m$ referring to entity $e$. These probabilities were estimated based on the frequencies of mention–entity pairs in the knowledge base. The example shows that the anchor text &lsquo;Sweden&rsquo; is most often used to refer to the entity [Sweden](http://en.wikipedia.org/wiki/Sweden), but in a few cases also to refer to Sweden&rsquo;s national football and ice hockey teams. Note that references are sorted in decreasing order of probability, so that the most probable pairing come first.\n",
    "\n",
    "Implement an entity linking method that resolves each mention to the most probable entity in the data frame. If the mention is not included in the data frame, you can predict the generic label `--NME--`. Print the precision, recall, and F1 of your method using the function that you implemented for Problem&nbsp;1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 40%\n",
      "recall: 32%\n",
      "F1: 36%\n"
     ]
    }
   ],
   "source": [
    "def mostprob(df):\n",
    "    for row in df.itertuples():\n",
    "        split = row[2].split(\" \")[row[3]:row[4]]\n",
    "        joined = \"_\".join(split)\n",
    "        entity = df_kb[df_kb[\"mention\"] == joined]\n",
    "        if(len(entity)>0):\n",
    "            pred_label = entity.iloc[0].entity\n",
    "        else:\n",
    "            pred_label = \"--NME--\"\n",
    "        yield (row[1], row[3], row[4], pred_label)\n",
    "\n",
    "mostprob_data = set(mostprob(res_df))\n",
    "evaluation_report(gold_new,mostprob_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Context-sensitive disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the entity mention &lsquo;Lincoln&rsquo;. The most probable entity for this mention turns out to be [Lincoln, Nebraska](http://en.wikipedia.org/Lincoln,_Nebraska); but in pages about American history, we would be better off to predict [Abraham Lincoln](http://en.wikipedia.org/Abraham_Lincoln). This suggests that we should try to disambiguate between different entity references based on the textual context on the page from which the mention was taken. Your task in this last problem is to implement this idea.\n",
    "\n",
    "Set up a dictionary that contains, for each mention $m$ that can refer to more than one entity $e$, a separate Naive Bayes classifier that is trained to predict the correct entity $e$, given the textual context of the mention. As the prior probabilities of the classifier, choose the probabilities $P(e|m)$ that you used in Problem&nbsp;5. To let you estimate the context-specific probabilities, we have compiled a data set with mention contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open('contexts.tsv.bz2') as source:\n",
    "    df_contexts = pd.read_csv(source, sep='\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data frame contains, for each ambiguous mention $m$ and each knowledge base entity $e$ to which this mention can refer, up to 100 randomly selected contexts in which $m$ is used to refer to $e$. For this data, a **context** is defined as the 5 tokens to the left and the 5 tokens to the right of the mention. Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>entity</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970</td>\n",
       "      <td>UEFA_Champions_League</td>\n",
       "      <td>Cup twice the first in @ and the second in 1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970</td>\n",
       "      <td>FIFA_World_Cup</td>\n",
       "      <td>America 1975 and during the @ and 1978 World C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990 World Cup</td>\n",
       "      <td>1990_FIFA_World_Cup</td>\n",
       "      <td>Manolo represented Spain at the @</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990 World Cup</td>\n",
       "      <td>1990_FIFA_World_Cup</td>\n",
       "      <td>Hašek represented Czechoslovakia at the @ and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990 World Cup</td>\n",
       "      <td>1990_FIFA_World_Cup</td>\n",
       "      <td>renovations in 1989 for the @ The present capa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          mention                 entity  \\\n",
       "0            1970  UEFA_Champions_League   \n",
       "1            1970         FIFA_World_Cup   \n",
       "2  1990 World Cup    1990_FIFA_World_Cup   \n",
       "3  1990 World Cup    1990_FIFA_World_Cup   \n",
       "4  1990 World Cup    1990_FIFA_World_Cup   \n",
       "\n",
       "                                             context  \n",
       "0    Cup twice the first in @ and the second in 1983  \n",
       "1  America 1975 and during the @ and 1978 World C...  \n",
       "2                 Manolo represented Spain at the @   \n",
       "3  Hašek represented Czechoslovakia at the @ and ...  \n",
       "4  renovations in 1989 for the @ The present capa...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_contexts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in each context, the position of the mention is indicated by the `@` symbol.\n",
    "\n",
    "From this data frame, it is easy to select the data that you need to train the classifiers – the contexts and corresponding entities for all mentions. To illustrate this, the following cell shows how to select all contexts that belong to the mention &lsquo;Lincoln&rsquo;:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41465    Nebraska Concealed Handgun Permit In @ municip...\n",
       "41466    Lazlo restaurants are located in @ and Omaha C...\n",
       "41467    California Washington Overland Park Kansas @ N...\n",
       "41468    City Missouri Omaha Nebraska and @ Nebraska It...\n",
       "41469    by Sandhills Publishing Company in @ Nebraska USA\n",
       "                               ...                        \n",
       "41609                                      @ Leyton Orient\n",
       "41610                    English division three Swansea @ \n",
       "41611    league membership narrowly edging out @ on goa...\n",
       "41612                                          @ Cambridge\n",
       "41613                                                   @ \n",
       "Name: context, Length: 149, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_contexts.context[df_contexts.mention == 'Lincoln']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the context-sensitive disambiguation method and evaluate its performance. Here are some more hints that may help you along the way:\n",
    "\n",
    "**Hint 1:** The prior probabilities for a Naive Bayes classifier can be specified using the `class_prior` option. You will have to provide the probabilities in the same order as the alphabetically sorted class (entity) names.\n",
    "\n",
    "**Hint 2:** Not all mentions in the knowledge base are ambiguous, and therefore not all mentions have context data. If a mention has only one possible entity, pick that one. If a mention has no entity at all, predict the `--NME--` label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Set up a dictionary that contains a seperate Naive Bayes classifier,\n",
    "# for each mention 𝑚 that can refer to more than one entity 𝑒,\n",
    "classifier_dict = {}\n",
    "\n",
    "for mention in set(df_contexts['mention']):\n",
    "    entities = set(df_contexts.entity[df_contexts.mention == mention])\n",
    "    if len(entities) > 1:\n",
    "        contexts, y = np.array([]), []\n",
    "        for ent in entities:\n",
    "            c = df_contexts.context[df_contexts.entity == ent]\n",
    "            contexts = np.append(contexts, np.array(c))\n",
    "            y += [ent for i in range(len(c))]\n",
    "\n",
    "        x = contexts.flatten()\n",
    "        \n",
    "    probabilities = df_kb.loc[df_kb.mention == mention].sort_values(by=['entity']).prob\n",
    "    pipe = Pipeline([('count_vectorizer', CountVectorizer()), ('classifier', MultinomialNB(class_prior=probabilities.values))])\n",
    "    pipe.fit(x, y)\n",
    "    classifier_dict[mention] = pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should expect to see a small (around 1&nbsp;unit) increase in both precision, recall, and F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 43%\n",
      "recall: 34%\n",
      "F1: 38%\n"
     ]
    }
   ],
   "source": [
    "def most_prob_nb(df):\n",
    "    for row in df.itertuples():\n",
    "        split_ent = row[2].split(\" \")[row[3]:row[4]]\n",
    "        joined_ent = \"_\".join(split_ent)\n",
    "        entity = df_kb[df_kb[\"mention\"] == joined_ent]\n",
    "            \n",
    "        if (len(entity) == 1):\n",
    "            pred_label = entity.iloc[0].entity\n",
    "        elif (len(entity) > 1):\n",
    "            split_sent = row[2].split(\" \")\n",
    "            left, right = split_sent[:row[3]][-5:], split_sent[row[4]:][:5]\n",
    "            context = \" \".join(left+right)\n",
    "            pred_label = classifier_dict[joined_ent].predict([context])[0]\n",
    "        else:\n",
    "            pred_label = \"--NME--\"\n",
    "        \n",
    "        yield (row[1], row[3], row[4], pred_label)\n",
    "\n",
    "most_prob_data = set(most_prob_nb(res_df))\n",
    "evaluation_report(gold_new, most_prob_data)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflection questions\n",
    "The following reflection questions are questions that you could be asked in the oral exam. Try to answer each of them in the form of a short text and enter it in the cell below. You will get feedback on your answers from your lab assistant.\n",
    "\n",
    "RQ 5.1: In Problem 3, you did an error analysis on the task of recognizing text spans mentioning named entities. Summarize your results. Pick one type of error that you observed. How could you improve the model’s performance on this type of error? What resources (such as domain knowledge, data, compute) would you need to implement this improvement?\n",
    ">In the results many of the errors where different kinds of numbers, dates or ordinals. To improve the performance and eliminate this error, we can use the filtering methods used in this lab. We can also use domain specific data to fine tune this model and retrain it to be more useful on our data.\n",
    "\n",
    "RQ 5.2: Thinking back about Problem 6, explain what the word context refers to in the task addressed there, and how context can help to disambiguate between different entities. Suggest other types of context that you could use for disambiguation.\n",
    ">The word context in problem 6 refers to the five tokens before and after the given entity. A mention can refer to many different entities, for example; 'Washington' can refer to the city, the state or even the president. By looking at the context and finding words like 'city', 'D.C', 'George' or 'state' it becomes possible to disambiguate the entity. It is in a sense using the idea that we can \"know a word by the company it keeps\".\n",
    "Instead of looking at the semantic context it appears in, it is also possible to look at the syntactic context. For example, looking at what part of speech tag it is and what part of speech tags that surround it.\n",
    "\n",
    "RQ 5.3: One type of entity mentions that we did not cover explicitly in this lab are pronouns. As an example, consider the sentence pair Ruth Bader Ginsburg was an American jurist. She served as an associate justice of the Supreme Court from 1993 until her death in 2020. What facts would you want to extract from this sentence pair? How do pronouns make fact extraction hard?\n",
    ">The fact that Ruth Bader Ginsburg is the one who served as an associate justice. However, since she is replaced with the word 'she' in the second sentence, it becomes impossible to now who it is if we only look at one sentence at a time. To counter this, there is a need to keep track of several sentences at all times. But many times the sentences are independent from each other as well. Thus, it becomes difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    Please read the section ‘General information’ on the ‘Labs’ page of the course website before submitting this notebook!\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
